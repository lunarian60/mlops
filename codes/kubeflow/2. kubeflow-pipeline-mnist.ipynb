{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ecd8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_REGION = 'ap-northeast-2'\n",
    "S3_BUCKET = 'mlops-kubernetes'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e302f31",
   "metadata": {},
   "source": [
    "Run the following command line if your bucket has not been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d98b8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 mb s3://$S3_BUCKET --region $AWS_REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbff0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca75c31",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "Download mnist data from sagemaker samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee79b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_data_bucket = f\"sagemaker-sample-files\"\n",
    "downloaded_data_prefix = \"datasets/image/MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5563131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.67 s, sys: 1.2 s, total: 2.87 s\n",
      "Wall time: 4.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle, gzip, numpy, json\n",
    "\n",
    "# Load the dataset\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(downloaded_data_bucket, f\"{downloaded_data_prefix}/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "with gzip.open(\"mnist.pkl.gz\", \"rb\") as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88665e24",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "prepare the datasets, train, test and validation sets and upload the train and test datasets to the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e114ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data will be uploaded to: s3://mlops-kubernetes/mnist_kmeans_example/train_data\n",
      "Test data will be uploaded to: s3://mlops-kubernetes/mnist_kmeans_example/test_data\n"
     ]
    }
   ],
   "source": [
    "import pickle, gzip, numpy, urllib.request, json\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Load the dataset\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "\n",
    "# Upload dataset to S3\n",
    "from sagemaker.amazon.common import write_numpy_to_dense_tensor\n",
    "import io\n",
    "import boto3\n",
    "\n",
    "###################################################################\n",
    "# This is the only thing that you need to change to run this code \n",
    "# Give the name of your S3 bucket \n",
    "bucket = S3_BUCKET \n",
    "\n",
    "# If you are gonna use the default values of the pipeline then \n",
    "# give a bucket name which is in us-west-2 region \n",
    "###################################################################\n",
    "\n",
    "train_data_key = 'mnist_kmeans_example/train_data'\n",
    "test_data_key = 'mnist_kmeans_example/test_data'\n",
    "train_data_location = 's3://{}/{}'.format(bucket, train_data_key)\n",
    "test_data_location = 's3://{}/{}'.format(bucket, test_data_key)\n",
    "print('Training data will be uploaded to: {}'.format(train_data_location))\n",
    "print('Test data will be uploaded to: {}'.format(test_data_location))\n",
    "\n",
    "# Convert the training data into the format required by the SageMaker KMeans algorithm\n",
    "buf = io.BytesIO()\n",
    "write_numpy_to_dense_tensor(buf, train_set[0], train_set[1])\n",
    "buf.seek(0)\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).Object(train_data_key).upload_fileobj(buf)\n",
    "\n",
    "# Convert the test data into the format required by the SageMaker KMeans algorithm\n",
    "write_numpy_to_dense_tensor(buf, test_set[0], test_set[1])\n",
    "buf.seek(0)\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).Object(test_data_key).upload_fileobj(buf)\n",
    "\n",
    "# Convert the valid data into the format required by the SageMaker KMeans algorithm\n",
    "numpy.savetxt('valid-data.csv', valid_set[0], delimiter=',', fmt='%g')\n",
    "s3_client = boto3.client('s3')\n",
    "input_key = \"{}/valid_data.csv\".format(\"mnist_kmeans_example/input\")\n",
    "s3_client.upload_file('valid-data.csv', bucket, input_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e05487",
   "metadata": {},
   "source": [
    "#### note: make sure you have done the followings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6aa688",
   "metadata": {},
   "source": [
    "> Typically in a production environment, you would assign fine-grained permissions depending on the nature of actions you take and leverage tools like [IAM Role for Service Account](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) for securing access to AWS resources but for simplicity we will assign AmazonSageMakerFullAccess and AmazonS3FullAccess IAM policy.  You can read more about granular policies [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) \n",
    "\n",
    "> In order to run this pipeline, we need two levels of IAM permissions\n",
    "\n",
    "> a) create Kubernetes secrets **aws-secret** with Sagemaker and S3 policies. Please make sure to create `aws-secret` in kubeflow namespace.\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: aws-secret\n",
    "  namespace: kubeflow\n",
    "type: Opaque\n",
    "data:\n",
    "  AWS_ACCESS_KEY_ID: YOUR_BASE64_ACCESS_KEY\n",
    "  AWS_SECRET_ACCESS_KEY: YOUR_BASE64_SECRET_ACCESS\n",
    "```\n",
    "> Note: To get base64 string, try `echo -n $AWS_ACCESS_KEY_ID | base64`\n",
    "\n",
    "> b) create an IAM execution role for Sagemaker and S3 so that the job can assume this role in order to perform Sagemaker and S3 actions. Make a note of this role as you will need it during pipeline creation step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43884e90",
   "metadata": {},
   "source": [
    "### Build SageMaker pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01edcbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import components\n",
    "from kfp import dsl\n",
    "from kfp.aws import use_aws_secret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9212f7e8",
   "metadata": {},
   "source": [
    "Load reusable sagemaker components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "730e4120",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_train_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/942be78bfe0f063084a5a006b3310b811a39f1ec/components/aws/sagemaker/train/component.yaml')\n",
    "sagemaker_model_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/942be78bfe0f063084a5a006b3310b811a39f1ec/components/aws/sagemaker/model/component.yaml')\n",
    "sagemaker_deploy_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/942be78bfe0f063084a5a006b3310b811a39f1ec/components/aws/sagemaker/deploy/component.yaml')\n",
    "sagemaker_batch_transform_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/942be78bfe0f063084a5a006b3310b811a39f1ec/components/aws/sagemaker/batch_transform/component.yaml')\n",
    "sagemaker_hpo_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/942be78bfe0f063084a5a006b3310b811a39f1ec/components/aws/sagemaker/hyperparameter_tuning/component.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3fa998",
   "metadata": {},
   "source": [
    "We will create a training job first. Once training job is done, it will persist trained model to S3. \n",
    "\n",
    "Then a job will be kicked off to create a `Model` manifest in Sagemaker. \n",
    "\n",
    "With this model, batch transformation job can use it to predict on other datasets, prediction service can create an endpoint using it.\n",
    "\n",
    "\n",
    "> Note: remember to use pass your **role_arn** to successfully run the job.\n",
    "\n",
    "> Note: If you use a different region, please replace `us-west-2` with your region. \n",
    "\n",
    "> Note: ECR Images for k-means algorithm\n",
    "\n",
    "|Region| ECR Image|\n",
    "|------|----------|\n",
    "|ap-northeast-2|835164637446.dkr.ecr.ap-northeast-2.amazonaws.com|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "527c62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_PIPELINE_PATH='s3://{}/mnist_kmeans_example'.format(S3_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad719a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://mlops-kubernetes/mnist_kmeans_example'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S3_PIPELINE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "650915a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your Sagemaker execution role.\n",
    "SAGEMAKER_ROLE_ARN='arn:aws:iam::145081548823:role/sagemaker-kfp-role'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb8873b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='MNIST Classification pipeline',\n",
    "    description='MNIST Classification using KMEANS in SageMaker'\n",
    ")\n",
    "def mnist_classification(region='ap-northeast-2',\n",
    "    image='835164637446.dkr.ecr.ap-northeast-2.amazonaws.com/kmeans:1',\n",
    "    training_input_mode='File',\n",
    "    hpo_strategy='Bayesian',\n",
    "    hpo_metric_name='test:msd',\n",
    "    hpo_metric_type='Minimize',\n",
    "    hpo_early_stopping_type='Off',\n",
    "    hpo_static_parameters='{\"k\": \"10\", \"feature_dim\": \"784\"}',\n",
    "    hpo_integer_parameters='[{\"Name\": \"mini_batch_size\", \"MinValue\": \"500\", \"MaxValue\": \"600\"}, {\"Name\": \"extra_center_factor\", \"MinValue\": \"10\", \"MaxValue\": \"20\"}]',\n",
    "    hpo_continuous_parameters='[]',\n",
    "    hpo_categorical_parameters='[{\"Name\": \"init_method\", \"Values\": [\"random\", \"kmeans++\"]}]',\n",
    "    hpo_channels='[{\"ChannelName\": \"train\", \\\n",
    "                \"DataSource\": { \\\n",
    "                    \"S3DataSource\": { \\\n",
    "                        \"S3Uri\": \"' + S3_PIPELINE_PATH + '/train_data\",  \\\n",
    "                        \"S3DataType\": \"S3Prefix\", \\\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                        } \\\n",
    "                    }, \\\n",
    "                \"ContentType\": \"\", \\\n",
    "                \"CompressionType\": \"None\", \\\n",
    "                \"RecordWrapperType\": \"None\", \\\n",
    "                \"InputMode\": \"File\"}, \\\n",
    "               {\"ChannelName\": \"test\", \\\n",
    "                \"DataSource\": { \\\n",
    "                    \"S3DataSource\": { \\\n",
    "                        \"S3Uri\": \"' + S3_PIPELINE_PATH + '/test_data\", \\\n",
    "                        \"S3DataType\": \"S3Prefix\", \\\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                        } \\\n",
    "                    }, \\\n",
    "                \"ContentType\": \"\", \\\n",
    "                \"CompressionType\": \"None\", \\\n",
    "                \"RecordWrapperType\": \"None\", \\\n",
    "                \"InputMode\": \"File\"}]',\n",
    "    hpo_spot_instance='False',\n",
    "    hpo_max_wait_time='3600',\n",
    "    hpo_checkpoint_config='{}',\n",
    "    output_location=S3_PIPELINE_PATH + '/output',\n",
    "    output_encryption_key='',\n",
    "    instance_type='ml.m5.2xlarge',\n",
    "    instance_count='1',\n",
    "    volume_size='50',\n",
    "    hpo_max_num_jobs='9',\n",
    "    hpo_max_parallel_jobs='2',\n",
    "    max_run_time='3600',\n",
    "    endpoint_url='',\n",
    "    network_isolation='True',\n",
    "    traffic_encryption='False',\n",
    "    train_channels='[{\"ChannelName\": \"train\", \\\n",
    "                \"DataSource\": { \\\n",
    "                    \"S3DataSource\": { \\\n",
    "                        \"S3Uri\": \"' + S3_PIPELINE_PATH + '/train_data\",  \\\n",
    "                        \"S3DataType\": \"S3Prefix\", \\\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\" \\\n",
    "                        } \\\n",
    "                    }, \\\n",
    "                \"ContentType\": \"\", \\\n",
    "                \"CompressionType\": \"None\", \\\n",
    "                \"RecordWrapperType\": \"None\", \\\n",
    "                \"InputMode\": \"File\"}]',\n",
    "    train_spot_instance='False',\n",
    "    train_max_wait_time='3600',\n",
    "    train_checkpoint_config='{}',\n",
    "    batch_transform_instance_type='ml.m4.xlarge',\n",
    "    batch_transform_input=S3_PIPELINE_PATH + '/input',\n",
    "    batch_transform_data_type='S3Prefix',\n",
    "    batch_transform_content_type='text/csv',\n",
    "    batch_transform_compression_type='None',\n",
    "    batch_transform_ouput=S3_PIPELINE_PATH + '/output',\n",
    "    batch_transform_max_concurrent='4',\n",
    "    batch_transform_max_payload='6',\n",
    "    batch_strategy='MultiRecord',\n",
    "    batch_transform_split_type='Line',\n",
    "    role_arn=SAGEMAKER_ROLE_ARN\n",
    "    ):\n",
    "\n",
    "    hpo = sagemaker_hpo_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        image=image,\n",
    "        training_input_mode=training_input_mode,\n",
    "        strategy=hpo_strategy,\n",
    "        metric_name=hpo_metric_name,\n",
    "        metric_type=hpo_metric_type,\n",
    "        early_stopping_type=hpo_early_stopping_type,\n",
    "        static_parameters=hpo_static_parameters,\n",
    "        integer_parameters=hpo_integer_parameters,\n",
    "        continuous_parameters=hpo_continuous_parameters,\n",
    "        categorical_parameters=hpo_categorical_parameters,\n",
    "        channels=hpo_channels,\n",
    "        output_location=output_location,\n",
    "        output_encryption_key=output_encryption_key,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=instance_count,\n",
    "        volume_size=volume_size,\n",
    "        max_num_jobs=hpo_max_num_jobs,\n",
    "        max_parallel_jobs=hpo_max_parallel_jobs,\n",
    "        max_run_time=max_run_time,\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=hpo_spot_instance,\n",
    "        max_wait_time=hpo_max_wait_time,\n",
    "        checkpoint_config=hpo_checkpoint_config,\n",
    "        role=role_arn,\n",
    "    ).apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY'))\n",
    "\n",
    "    training = sagemaker_train_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        image=image,\n",
    "        training_input_mode=training_input_mode,\n",
    "        hyperparameters=hpo.outputs['best_hyperparameters'],\n",
    "        channels=train_channels,\n",
    "        instance_type=instance_type,\n",
    "        instance_count=instance_count,\n",
    "        volume_size=volume_size,\n",
    "        max_run_time=max_run_time,\n",
    "        model_artifact_path=output_location,\n",
    "        output_encryption_key=output_encryption_key,\n",
    "        network_isolation=network_isolation,\n",
    "        traffic_encryption=traffic_encryption,\n",
    "        spot_instance=train_spot_instance,\n",
    "        max_wait_time=train_max_wait_time,\n",
    "        checkpoint_config=train_checkpoint_config,\n",
    "        role=role_arn,\n",
    "    ).apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY'))\n",
    "\n",
    "    create_model = sagemaker_model_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        model_name=training.outputs['job_name'],\n",
    "        image=training.outputs['training_image'],\n",
    "        model_artifact_url=training.outputs['model_artifact_url'],\n",
    "        network_isolation=network_isolation,\n",
    "        role=role_arn\n",
    "    ).apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY'))\n",
    "\n",
    "    prediction = sagemaker_deploy_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        model_name_1=create_model.output,\n",
    "    ).apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY'))\n",
    "\n",
    "    batch_transform = sagemaker_batch_transform_op(\n",
    "        region=region,\n",
    "        endpoint_url=endpoint_url,\n",
    "        model_name=create_model.output,\n",
    "        instance_type=batch_transform_instance_type,\n",
    "        instance_count=instance_count,\n",
    "        max_concurrent=batch_transform_max_concurrent,\n",
    "        max_payload=batch_transform_max_payload,\n",
    "        batch_strategy=batch_strategy,\n",
    "        input_location=batch_transform_input,\n",
    "        data_type=batch_transform_data_type,\n",
    "        content_type=batch_transform_content_type,\n",
    "        split_type=batch_transform_split_type,\n",
    "        compression_type=batch_transform_compression_type,\n",
    "        output_location=batch_transform_ouput\n",
    "    ).apply(use_aws_secret('aws-secret', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b99432",
   "metadata": {},
   "source": [
    "### Compile your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f2b8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(mnist_classification, 'mnist-kmeans-pipeline.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e48ff9",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Open Sagemaker console and find your endpoint name. Please check dataset section to get train_set.\n",
    "\n",
    "Once your pipeline is done, you can find sagemaker endpoint name and replace `ENDPOINT_NAME` value with your newly created endpoint name. \n",
    "\n",
    "\n",
    "> Note: make sure to attach `sagemaker:InvokeEndpoint` to the worker node nodegroup that is running this jupyter notebook.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sagemaker:InvokeEndpoint\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c19343f",
   "metadata": {},
   "source": [
    "## Find your Endpoint name in AWS Console\n",
    "\n",
    "Open AWS console and enter Sagemaker service, find the endpoint name as the following picture shows.\n",
    "\n",
    "![download-pipeline](./images/sm-endpoint.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c4e6c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the endpoint name with yours.\n",
    "ENDPOINT_NAME='Endpoint-20211108100713-NI4G'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd001cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'closest_cluster': 0.0, 'distance_to_cluster': 7.282129764556885}]}\n"
     ]
    }
   ],
   "source": [
    "# Simple function to create a csv from our numpy array\n",
    "def np2csv(arr):\n",
    "    csv = io.BytesIO()\n",
    "    numpy.savetxt(csv, arr, delimiter=',', fmt='%g')\n",
    "    return csv.getvalue().decode().rstrip()\n",
    "\n",
    "runtime = boto3.Session(region_name='ap-northeast-2').client('sagemaker-runtime')\n",
    "\n",
    "payload = np2csv(train_set[0][30:31])\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                   ContentType='text/csv',\n",
    "                                   Body=payload)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c9990b",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "\n",
    "Go to Sagemaker console and delete `endpoint`, `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd93d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
